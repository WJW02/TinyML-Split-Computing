{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Download and extract the dataset\n",
    "dataset = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "directory = tf.keras.utils.get_file('flower_photos', origin=dataset, untar=True)\n",
    "data_directory = pathlib.Path(directory)"
   ],
   "id": "b70706ff798714e2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:28:44.999813Z",
     "start_time": "2024-09-05T10:27:28.019524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the image size and batch size\n",
    "img_height, img_width = 180, 180\n",
    "batch_size = 32\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_directory,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Get the class names\n",
    "classnames = train_ds.class_names\n",
    "\n",
    "# Create the ResNet50 model and set the layers to be non-trainable\n",
    "resnet_model = tf.keras.Sequential()\n",
    "pretrained_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    input_shape=(img_height, img_width, 3),\n",
    "    pooling='avg',\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    layer.trainable = False\n",
    "resnet_model.add(pretrained_model)\n",
    "\n",
    "# Add fully connected layers for classification\n",
    "resnet_model.add(layers.Flatten())\n",
    "resnet_model.add(layers.Dense(512, activation='relu'))\n",
    "resnet_model.add(layers.Dense(len(classnames), activation='softmax'))\n",
    "\n",
    "# Compile and train the model using sparse_categorical_crossentropy\n",
    "resnet_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = resnet_model.fit(train_ds, validation_data=validation_ds, epochs=2)\n"
   ],
   "id": "d52b246ff5aa5499",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:28:46.487711Z",
     "start_time": "2024-09-05T10:28:45.000906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Model Inference\n",
    "# Load and preprocess the sample image\n",
    "\n",
    "frame_path = ['data/rose.jpg', 'data/sunflower.jpg', 'data/tulip.jpg',\n",
    "              'data/dandelion.jpg']  # Replace with the path to your image\n",
    "for frame_path in frame_path:\n",
    "    image = cv2.imread(frame_path)\n",
    "    image_resized = cv2.resize(image, (img_height, img_width))  # Resize the image\n",
    "    image = np.expand_dims(image_resized, axis=0)  # Expand dimensions for batch size\n",
    "\n",
    "    # Normalize the image to match the training data\n",
    "    image = image / 255.0  # Rescale the image to [0, 1] if needed\n",
    "\n",
    "    # Make predictions\n",
    "    image_pred = resnet_model.predict(image)\n",
    "\n",
    "    # Produce a human-readable output label\n",
    "    image_output_class = classnames[np.argmax(image_pred)]\n",
    "    print(\"The predicted class is\", image_output_class)\n"
   ],
   "id": "460f9cffb3c88e93",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T10:28:51.334190Z",
     "start_time": "2024-09-05T10:28:50.859408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the model in TensorFlow's SavedModel format\n",
    "resnet_model.save('./resnet_model/resnet_model.keras')"
   ],
   "id": "c6d83e63564f4b71",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:02:00.436326Z",
     "start_time": "2024-09-08T18:01:57.638698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define your image processing parameters\n",
    "img_height, img_width = 224, 224  # Ensure these match the model's expected input dimensions\n",
    "\n",
    "# List of image paths\n",
    "frame_paths = ['data/rose.jpg', 'data/sunflower.jpg', 'data/tulip.jpg', 'data/dandelion.jpg']\n",
    "\n",
    "# Load the original model\n",
    "model = tf.keras.models.load_model('./resnet_model.keras')\n",
    "\n",
    "# Create a list to hold the submodels\n",
    "submodels = []\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "# Iterate through layers and create submodels\n",
    "x = input_tensor  # Start with the input tensor\n",
    "for layer in model.layers:\n",
    "    # Pass the input through the current layer to build the submodel\n",
    "    x = layer(x)\n",
    "    submodel = Model(inputs=input_tensor, outputs=x)\n",
    "    submodels.append(submodel)\n",
    "\n",
    "# Loop over the images and process each one through the submodels\n",
    "for frame_path in frame_paths:\n",
    "    # Read and preprocess the image\n",
    "    image = cv2.imread(frame_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {frame_path}\")\n",
    "        continue\n",
    "\n",
    "    # Resize the image to the correct dimensions\n",
    "    image_resized = cv2.resize(image, (img_width, img_height))\n",
    "    image_resized = np.expand_dims(image_resized, axis=0)  # Expand dimensions for batch size\n",
    "\n",
    "    # Normalize the image\n",
    "    image_resized = image_resized / 255.0  # Rescale the image to [0, 1]\n",
    "\n",
    "    # Pass the image through each submodel\n",
    "    for i, submodel in enumerate(submodels):\n",
    "        # Get the output from the current submodel\n",
    "        layer_output = submodel.predict(image_resized)\n",
    "        \n",
    "        # Handle outputs of different shapes\n",
    "        if len(layer_output.shape) == 4:  # If the output is an image-like tensor\n",
    "            output_image = layer_output[0, :, :, :]\n",
    "            # Normalize the output for visualization\n",
    "            output_image = (output_image - np.min(output_image)) / (np.max(output_image) - np.min(output_image))\n",
    "        elif len(layer_output.shape) == 2:  # Dense layers output 2D tensors\n",
    "            print(f\"Output of submodel {i} ({submodel.layers[-1].name}): {layer_output}\")\n",
    "        \n",
    "        # Update the input for the next submodel\n",
    "        image_resized = layer_output\n",
    "\n"
   ],
   "id": "9bdb0d8d35f0de69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1s/step\n",
      "Output of submodel 0 (resnet50): [[0.         0.         0.00494438 ... 2.1174688  0.         0.        ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_37\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 2048)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 48\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Pass the image through each submodel\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, submodel \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(submodels):\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;66;03m# Get the output from the current submodel\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[43msubmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_resized\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;66;03m# Handle outputs of different shapes\u001B[39;00m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(layer_output\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m:  \u001B[38;5;66;03m# If the output is an image-like tensor\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Git\\GitHub\\TinyML-Split-Computing\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Documents\\Git\\GitHub\\TinyML-Split-Computing\\.venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001B[0m, in \u001B[0;36massert_input_compatibility\u001B[1;34m(input_spec, inputs, layer_name)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m dim \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m spec_dim \u001B[38;5;241m!=\u001B[39m dim:\n\u001B[1;32m--> 245\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    246\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_index\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of layer \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    247\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mincompatible with the layer: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    248\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mspec\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    249\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    250\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Input 0 of layer \"functional_37\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 2048)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "506c3271887bfd58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
